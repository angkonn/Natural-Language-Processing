{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY1_MY6zdP89"
      },
      "source": [
        "## Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\88018\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\88018\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
            "   ------------- -------------------------- 0.5/1.5 MB 1.3 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 0.5/1.5 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.3/1.5 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 1.3 MB/s eta 0:00:00\n",
            "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
            "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Installing collected packages: regex, joblib, click, nltk\n",
            "\n",
            "   ---------------------------------------- 0/4 [regex]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [click]\n",
            "   -------------------- ------------------- 2/4 [click]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ---------------------------------------- 4/4 [nltk]\n",
            "\n",
            "Successfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\88018\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEWuftE1cE9a",
        "outputId": "ca4f31ac-dd9f-467e-bc37-cdcd823dc755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sent Tokenize output:['Hi John, How are you doing?', 'I will be travelling to your city.', 'Lets catchup']\n",
            "Word Tokenize output:['Hi', 'John', ',', 'How', 'are', 'you', 'doing', '?', 'I', 'will', 'be', 'travelling', 'to', 'your', 'city', '.', 'Lets', 'catchup']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "\n",
        "text = \"Hi John, How are you doing? I will be travelling to your city. Lets catchup\"\n",
        "\n",
        "print(f\"Sent Tokenize output:{sent_tokenize(text)}\")\n",
        "print(f\"Word Tokenize output:{word_tokenize(text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sswowkFycJQ1"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAGh-Aw9cJOF",
        "outputId": "5fa1a3ea-5ac3-48fb-8455-f61db9225eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "play\n",
            "play\n",
            "play\n",
            "increas\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "print(stemmer.stem(\"playing\"))\n",
        "print(stemmer.stem(\"plays\"))\n",
        "print(stemmer.stem(\"played\"))\n",
        "print(stemmer.stem(\"increases\")) #will produce a word that is not in dictionary. So Stemming is not always reliable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyFRcM3ycJK-"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\88018\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfhRMFe7dUa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "increase\n",
            "increasing\n",
            "increase\n",
            "increasing\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemm = WordNetLemmatizer()\n",
        "\n",
        "print(lemm.lemmatize(\"increases\"))\n",
        "print(lemm.lemmatize(\"increasing\"))\n",
        "print(lemm.lemmatize(\"increasing\", pos=\"v\")) #verb\n",
        "print(lemm.lemmatize(\"increasing\", pos=\"n\")) #noun"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parts Of Speech (POS) Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Sw899MHPdUYO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\88018\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\88018\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\88018/nltk_data'\n    - 'c:\\\\Python\\\\.conda\\\\nltk_data'\n    - 'c:\\\\Python\\\\.conda\\\\share\\\\nltk_data'\n    - 'c:\\\\Python\\\\.conda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\88018\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\88018\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\88018\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi John, How are you doing? I will be travelling to your city. Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms catchup\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[1;32m---> 14\u001b[0m tagged \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(tagged)\n",
            "File \u001b[1;32mc:\\Python\\.conda\\Lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
            "File \u001b[1;32mc:\\Python\\.conda\\Lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
            "File \u001b[1;32mc:\\Python\\.conda\\Lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python\\.conda\\Lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
            "File \u001b[1;32mc:\\Python\\.conda\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\88018/nltk_data'\n    - 'c:\\\\Python\\\\.conda\\\\nltk_data'\n    - 'c:\\\\Python\\\\.conda\\\\share\\\\nltk_data'\n    - 'c:\\\\Python\\\\.conda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\88018\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\88018\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\Users\\\\88018\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Manually set the nltk_data path\n",
        "nltk.data.path.append(r\"C:\\Users\\88018\\AppData\\Roaming\\nltk_data\")\n",
        "\n",
        "nltk.download('punkt', download_dir=r\"C:\\Users\\88018\\AppData\\Roaming\\nltk_data\")\n",
        "nltk.download('averaged_perceptron_tagger', download_dir=r\"C:\\Users\\88018\\AppData\\Roaming\\nltk_data\")\n",
        "\n",
        "# Test\n",
        "text = \"Hi John, How are you doing? I will be travelling to your city. Let's catchup\"\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "print(tagged)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dbuxWOadUVk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
